import streamlit as st
from streamlit_webrtc import webrtc_streamer, AudioProcessorBase, WebRtcMode, RTCConfiguration
import collections
import time
import speech_recognition as sr
import threading 
import av
import pydub
import numpy as np
import openai

r = sr.Recognizer() # ã‚°ãƒ­ãƒ¼ãƒãƒ«ã§Recognizerã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ç”Ÿæˆ

from backend_app_preparation import MeetingAssistant,TextEmbedder, TopicDeviationDetector, AudioRecoginizer

st.set_page_config(
    page_title="FlowLink",
    page_icon="ğŸ—£ï¸"
)

st.title("ğŸ—£ï¸FlowLink")
st.write("ä¼šè­°æ”¯æ´ãƒ„ãƒ¼ãƒ« ---ã©ã‚“ãªäººã‚‚ã‚¹ãƒ ãƒ¼ã‚ºãªä¼šè­°ã®é€²è¡Œã‚’")

if "recoginizer_thread_running" not in st.session_state:
    st.session_state.recoginizer_thread_running = False

class AudioProcessor(AudioProcessorBase):
    def __init__(self):
        self.buffer = []
    def recv(self, frame: av.AudioFrame) -> av.AudioFrame:
        audio = frame.to_ndarray()[0]
        self.buffer.append(audio)
        return frame  # ãƒ•ãƒ¬ãƒ¼ãƒ ã¯å¤‰æ›´ã—ãªã„


if 'topic_info' not in st.session_state:
    st.session_state.topic_info = None
if 'detector' not in st.session_state:
    st.session_state.detector = None
if 'all_utterances' not in st.session_state:
    st.session_state.all_utterances = collections.deque(maxlen=20)
if 'recoginition_queue' not in st.session_state: # recoginition_queueã‚’åˆæœŸåŒ–
    st.session_state.recoginition_queue = collections.deque()
if 'recognizer_thread_running' not in st.session_state:
    st.session_state.recognizer_thread_running = False


tab_titles = ['ä¼šè­°ã®æƒ…å ±','ä¼šè­°ãƒãƒ£ãƒƒãƒˆ','ç™ºè¨€è¨˜éŒ²']
tab1, tab2, tab3 = st.tabs(tab_titles)

with tab1:
    with st.expander("ä¼šè­°ã®æœ€çµ‚ç›®æ¨™", expanded=True):
        topic_thema = st.text_input("ä¼šè­°ã®ãƒ†ãƒ¼ãƒ", key="topic_thema")
        topic_text_input = st.text_input("ä¼šè­°ã®ç›®çš„", key="topic_input_area")
        set_topic_button = st.button("æ±ºå®š", key="set_topic_button")
        if set_topic_button or st.session_state.topic_info is None:
            if topic_text_input:
                topic_manager = MeetingAssistant()
                meeting_id = "current_meeting"
                topic_manager.add_topic(meeting_id, topic_text_input)
                st.session_state.topic_info = topic_manager.get_topic(meeting_id)

                @st.cache_resource
                #Detector ã«æ¸¡ã™ãŸã‚ã« TextEmbedder ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã¯ä½œã‚‰ãªã„ã€‚ãŸã ã—ã€topic_embedding ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã«ã¯ TextEmbedder ãŒå¿…è¦ãªã®ã§ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã¯ä½œæˆ
                def get_detector_instance(current_topic_info_dict):
                    embedder_instance = TextEmbedder()
                    topic_embedding =embedder_instance.get_embedding(current_topic_info_dict['topic_text'])
                    detector_instance = TopicDeviationDetector(
                        topic_embedding,  
                        current_topic_info_dict,       
                        similarity_threshold=0.8,
                        consecutive_deviations_needed=2,
                        cooldown_period_seconds=10
                    )
                    return detector_instance
                
                st.session_state.detector = get_detector_instance(st.session_state.topic_info)

                st.success(f"ä¼šè­°ã®æƒ…å ±ãŒè¨­å®šã•ã‚Œã¾ã—ãŸ: {st.session_state.topic_info['topic_text']}")
            else:
                st.warning("æƒ…å ±ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
with tab2:
    st.info("ãƒã‚¤ã‚¯ã‚’ã‚ªãƒ³ã«ã—ã¦ãã ã•ã„")

with tab3:
    
    st.write('éŸ³å£°èªè­˜ã‚’é–‹å§‹ã—ã¾ã™ã€‚ãƒã‚¤ã‚¯ã«è©±ã—ã‹ã‘ã¦ãã ã•ã„ã€‚') # 'test'ã‚’ã‚ˆã‚Šåˆ†ã‹ã‚Šã‚„ã™ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«
    webrtc_ctx = webrtc_streamer(
        key="audio-demo2",
        mode=WebRtcMode.SENDONLY,
        audio_frame_callback=AudioRecoginizer.audio_frame_callback_func, 
        media_stream_constraints={"video": False, "audio": True},
    )
    result_placeholder = st.empty()
   
    # if  webrtc_ctx.state.playing:
    #     if not st.session_state.recognizer_thread_running: 
    #         st.session_state.recognizer_thread_running = True
    #         # # threading.Threadã®targetã«ã¯rã‚’æ¸¡ã™å¿…è¦ãŒãªã„
    #         threading.Thread(target=AudioRecoginizer.recoginizer_thread, daemon=True).start()
    #         # audio_frames = webrtc_ctx.audio_receiver.get_frames(timeout=1)
    #     result_placeholder.info("ãƒã‚¤ã‚¯ãŒã‚ªãƒ³ã§ã™ã€‚ç™ºè¨€ã‚’èªè­˜ä¸­")
    # else:
    #     if st.session_state.recognizer_thread_running:
    #         st.session_state.recognizer_thread_running = False
    #     result_placeholder.write("ãƒã‚¤ã‚¯ãŒéŸ³å£°ã‚’èªè­˜ã—ã¦ã„ã¾ã›ã‚“")
    
    st.subheader("æœ€è¿‘ã®ç™ºè¨€å±¥æ­´")
    # ç™ºè¨€å±¥æ­´ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§è¡¨ç¤º
    if st.session_state.all_utterances:
        for i, utt in enumerate(reversed(list(st.session_state.all_utterances))):
            st.markdown(f"**{len(st.session_state.all_utterances) - i}.** {utt}")
    else:
        st.write("ã¾ã ç™ºè¨€ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
